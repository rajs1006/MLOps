{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1085ed",
   "metadata": {},
   "source": [
    "## LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9474c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.1 s (started: 2023-04-14 12:07:44 +02:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline \n",
    "#inline\n",
    "# %pdb\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "## setting path\n",
    "## set the base path\n",
    "# os.chdir(pathlib.Path(os.path.dirname(path[0])).parent.absolute())\n",
    "# ## set path to sys for Lmbda functio\n",
    "# sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05e1468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36mConnected to the ZenML server: \u001b[0m\u001b[2;32m'http://127.0.0.1:8237'\u001b[0m\n",
      "\u001b[2;36mRunning with active workspace: \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mrepository\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "\u001b[2K\u001b[2;36mActive repository stack set to: \u001b[0m\u001b[2;32m'ireland_green_deployment_stack'\u001b[0m.\n",
      "\u001b[2K\u001b[32mâ ™\u001b[0m Setting the repository active stack to 'ireland_green_deployment_stack'...oyment_stack'...\u001b[0m\n",
      "\u001b[1A\u001b[2Ktime: 4.31 s (started: 2023-03-27 13:49:08 +02:00)\n"
     ]
    }
   ],
   "source": [
    "!zenml stack set ireland_green_deployment_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14cb70",
   "metadata": {},
   "source": [
    "## IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ae32e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - /opt/conda/conda/envs/azba/lib/python3.9/site-packages/plotly/graph_objs/__init__.py:288: DeprecationWarning:\n",
      "\n",
      "distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\n",
      "\n",
      "WARNING:py.warnings:/opt/conda/conda/envs/azba/lib/python3.9/site-packages/plotly/graph_objs/__init__.py:288: DeprecationWarning:\n",
      "\n",
      "distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.25 s (started: 2023-03-27 13:49:12 +02:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "# import dtale\n",
    "# from prophet import Prophet\n",
    "from neuralprophet import NeuralProphet, set_log_level, set_random_seed\n",
    "set_log_level(\"ERROR\")\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from scipy.stats import t\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://0.0.0.0:7009\")\n",
    "# from mlflow import MlflowClient \n",
    "\n",
    "import dtale\n",
    "\n",
    "# from lightgbm import LGBMRegressor\n",
    "# import dtale\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import power_transform, scale\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from imblearn.ensemble import RUSBoostClassifier, EasyEnsembleClassifier, BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score\n",
    "from sklearn.metrics import explained_variance_score, max_error, r2_score, mean_absolute_error, f1_score\n",
    "\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsRegressor, NeighborhoodComponentsAnalysis\n",
    "# from lightgbm import LGBMClassifier\n",
    "# import lightgbm as lgb\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import VotingRegressor, GradientBoostingRegressor, RandomForestRegressor, VotingClassifier\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "from absl import logging as absl_logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "absl_logging.set_verbosity(-10000)\n",
    "\n",
    "import mlflow\n",
    "from zenml.steps import Output, step\n",
    "\n",
    "\n",
    "import sqlalchemy\n",
    "import yaml\n",
    "from typing import Dict, Union\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import feast\n",
    "from feast import FeatureStore\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import (\n",
    "    DataQualityPreset,\n",
    "    DataDriftPreset,\n",
    "    ClassificationPreset,\n",
    ")\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metrics.base_metric import generate_column_metrics\n",
    "from evidently.metric_preset import DataDriftPreset, TargetDriftPreset\n",
    "from evidently.metrics import *\n",
    "\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.tests.base_test import generate_column_tests\n",
    "from evidently.test_preset import DataStabilityTestPreset, NoTargetPerformanceTestPreset, DataQualityTestPreset\n",
    "from evidently.tests import *\n",
    "\n",
    "from evidently.metric_preset import (\n",
    "    DataQualityPreset,\n",
    "    DataDriftPreset,\n",
    "    ClassificationPreset,\n",
    ")\n",
    "from evidently.report import Report\n",
    "from zenml.integrations.mlflow.flavors.mlflow_experiment_tracker_flavor import (\n",
    "    MLFlowExperimentTrackerSettings,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00326566",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05d0ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 67.2 ms (started: 2023-03-27 13:58:26 +02:00)\n"
     ]
    }
   ],
   "source": [
    "def hourlySample(data):\n",
    "    d = data.copy()\n",
    "    d.index = pd.DatetimeIndex(d.index) \n",
    "    return d.reindex(pd.date_range(start=d.index.min(), periods = 24 * len(d), freq='H'), method='ffill').squeeze()\n",
    "\n",
    "def findOutliers(df):\n",
    "    s = df.copy()\n",
    "    q1 = s.quantile(0.25)\n",
    "    q3 = s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    iqr_lower = q1 - 8 * iqr\n",
    "    iqr_upper = q3 + 8 * iqr\n",
    "#     outliers = s[(s < iqr_lower) | (s > iqr_upper)]\n",
    "    s[s < iqr_lower] = iqr_lower\n",
    "    s[s > iqr_upper] = iqr_upper\n",
    "    \n",
    "    return s\n",
    "\n",
    "def _scores(pred, true):\n",
    "    display(confusion_matrix(true.loc[pred.index], pred))\n",
    "    print(\"bAcc : \", bscore := balanced_accuracy_score(true.loc[pred.index], pred))\n",
    "    print(\"acc :\" , acc := accuracy_score(true.loc[pred.index], pred))\n",
    "    print(\"f1 :\", f1 := f1_score(true.loc[pred.index], pred))\n",
    "    print(\"report :\", classification_report(true.loc[pred.index], pred))\n",
    "    print(\"precision, recall\", prScore := precision_recall_fscore_support(true.loc[pred.index], pred, average='weighted'))\n",
    "    \n",
    "    return (bscore, acc, f1, prScore)\n",
    "\n",
    "def _scoresArr(pred, true, printVal=True):\n",
    "    bscore = balanced_accuracy_score(true, pred)\n",
    "    acc = accuracy_score(true, pred)\n",
    "    f1 = f1_score(true, pred)\n",
    "    prScore = precision_recall_fscore_support(true, pred, average='weighted')\n",
    "        \n",
    "    if printVal:\n",
    "        display(confusion_matrix(true, pred))\n",
    "        print(\"bAcc : \", bscore)\n",
    "        print(\"acc :\" , acc)\n",
    "        print(\"f1 :\", f1)\n",
    "        print(\"report :\", classification_report(true, pred))\n",
    "        print(\"precision, recall\", )\n",
    "    \n",
    "    return (bscore, acc, f1, prScore)\n",
    "\n",
    "def _scoresRegArr(actual, pred, printVal=True):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "        \n",
    "    if printVal:\n",
    "        print(\"rmse : \", rmse)\n",
    "        print(\"mae : \", mae)\n",
    "        print(\"r2 : \", r2)\n",
    "        \n",
    "    return (rmse, mae, r2)\n",
    "    \n",
    "def _toMultiIndex(d, name=None):\n",
    "    reformed_dict = {}\n",
    "    for outerKey, innerDict in d.items():\n",
    "        for innerKey, values in innerDict.items():\n",
    "            reformed_dict[(outerKey,\n",
    "                           innerKey)] = values\n",
    "\n",
    "    l = pd.DataFrame(reformed_dict)\n",
    "    display(l)\n",
    "    if name:\n",
    "        l.to_csv(name + \".csv\")\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2142e16",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad4dba1",
   "metadata": {},
   "source": [
    "### Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19849b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 36 ms (started: 2023-03-27 13:49:17 +02:00)\n"
     ]
    }
   ],
   "source": [
    "@step\n",
    "def getDates() -> Output(\n",
    "    stDate = str, endDate=str\n",
    "):\n",
    "    stDate = '2022-07-01'\n",
    "    endDate = '2023-02-21'\n",
    "    \n",
    "    return stDate, endDate\n",
    "\n",
    "@step\n",
    "def fetchData(_stDate:str, _endDate:str)-> Output(dataFrame=pd.DataFrame):\n",
    "\n",
    "    dd = [{'epoch_time' : int(pd.Timestamp(d.strftime('%Y-%m-%d %H:%M:%S')).timestamp())} for d in pd.date_range(start=_stDate, end=_endDate, freq='H')]\n",
    "    \n",
    "    repoPath = \"<path of repo>\"\n",
    "    store = FeatureStore(repo_path=repoPath)\n",
    "\n",
    "    batch_features = store.get_online_features(\n",
    "        entity_rows=dd,\n",
    "        features=store.get_feature_service(\"ireland_data_service\", allow_cache=True),\n",
    "    ).to_df()\n",
    "    \n",
    "    df = batch_features.copy()\n",
    "    \n",
    "    df.set_index('epoch_time', inplace=True)\n",
    "    df.index = pd.to_datetime(df.index, unit='s')\n",
    "    df.index.name = None\n",
    "    \n",
    "    dataFrame = df.astype(float)\n",
    "    \n",
    "    # drop the first NaN value as that might be because of shift\n",
    "    dataFrame = dataFrame.iloc[dataFrame.notnull().all(axis=1).argmax() :]\n",
    "    dataFrame = dataFrame.groupby(by=dataFrame.index, as_index=True).mean()\n",
    "    \n",
    "    dataFrame = dataFrame.reindex(\n",
    "        pd.date_range(\n",
    "            start=dataFrame.index.min(), end=dataFrame.index.max(), freq=\"H\"\n",
    "        ),\n",
    "        fill_value=np.nan,\n",
    "    )\n",
    "    dataFrame = dataFrame.interpolate(method=\"bfill\", axis=0)\n",
    "    \n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf40af0",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4d164e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.8 ms (started: 2023-03-27 13:49:17 +02:00)\n"
     ]
    }
   ],
   "source": [
    "@step\n",
    "def filteredData(\n",
    "    dataConcatenated: pd.DataFrame, _stDate: str\n",
    ") -> Output(targetSpreadWOOutlier=pd.DataFrame, features=pd.DataFrame):\n",
    "    data = dataConcatenated.loc[_stDate:][:-1]\n",
    "    \n",
    "    targetSpread = data[\"price_diff\"].dropna().asfreq(\"h\")\n",
    "    features = data.loc[\n",
    "        targetSpread.index, ~data.columns.isin([\"price_diff\", \"ie_actual_wnd\"])\n",
    "    ].asfreq(\"h\")\n",
    "\n",
    "    targetSpreadWOOutlier = findOutliers(targetSpread).to_frame()\n",
    "    \n",
    "    features = features.loc[: , ['day', 'weekday', 'year_day', 'week', 'month', 'quarter', 'lag1',\n",
    "       'lag2', 'lag3', 'lag4', 'lag5', 'lag6', 'lag7', 'lag30']]\n",
    "    \n",
    "    return targetSpreadWOOutlier, features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000fc6f",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd163fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 171 ms (started: 2023-03-27 13:56:28 +02:00)\n"
     ]
    }
   ],
   "source": [
    "@step\n",
    "def prepareFeatures(features : pd.DataFrame, _stDate:str, _endDate:str)-> Output(\n",
    "        X_test=pd.DataFrame, X_day_test=pd.DataFrame\n",
    "    ):\n",
    "    \n",
    "    features = features.dropna()\n",
    "    dayFeatures = features.resample('D', axis=0).sum() \n",
    "    \n",
    "    featureColumns = list(features.columns)\n",
    "    \n",
    "    X_test = features.loc[_stDate : _endDate, featureColumns]\n",
    "    X_day_test = features.loc[_stDate : _endDate, featureColumns].resample('D', axis=0).sum()\n",
    "    \n",
    "    return X_test, X_day_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7fcd42",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f441c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.3 ms (started: 2023-03-27 13:49:17 +02:00)\n"
     ]
    }
   ],
   "source": [
    "@step\n",
    "def saveDataToCsv(preds : list, trues: list)->pd.DataFrame:\n",
    "    pred1 = pd.DataFrame(preds[0], index=trues[0].index)\n",
    "    pred2 = hourlySample(pd.DataFrame(preds[1], index=trues[1]))\n",
    "    \n",
    "    df = pd.concat([pred1, pred2], axis=1)\n",
    "    df.to_csv('predictions.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0142dac8",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3305600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 60.6 ms (started: 2023-03-27 13:49:17 +02:00)\n"
     ]
    }
   ],
   "source": [
    "from zenml.services import BaseService\n",
    "from zenml.client import Client\n",
    "\n",
    "@step(enable_cache=False)\n",
    "def prediction_service_loader() -> Output(\n",
    "    trendService=BaseService, dayService=BaseService\n",
    "):\n",
    "    \"\"\"Load the model service of our train_evaluate_deploy_pipeline.\"\"\"\n",
    "    client = Client()\n",
    "    model_deployer = client.active_stack.model_deployer\n",
    "    trendServices = model_deployer.find_model_server(\n",
    "        pipeline_name=\"ireland_eval\",\n",
    "        pipeline_step_name=\"trendModelDeployer\",\n",
    "        running=True,\n",
    "    )\n",
    "    \n",
    "    dayServices = model_deployer.find_model_server(\n",
    "        pipeline_name=\"ireland_eval\",\n",
    "        pipeline_step_name=\"dayModelDeployer\",\n",
    "        running=True,\n",
    "    )\n",
    "    \n",
    "    trendService = trendServices[0]\n",
    "    dayService = dayServices[0]\n",
    "    \n",
    "    return trendService, dayService\n",
    "\n",
    "@step\n",
    "def predictor(\n",
    "    trendService: BaseService,\n",
    "    dayService: BaseService,\n",
    "    data: pd.DataFrame,\n",
    "    dayData: pd.DataFrame,\n",
    ") -> Output(predictions=list, true=list):\n",
    "    \"\"\"Run a inference request against a prediction service\"\"\"\n",
    "    \n",
    "    trendService.start(timeout=100)  # should be a NOP if already started\n",
    "    trendPrediction = trendService.predict(data.to_numpy())\n",
    "\n",
    "    dayService.start(timeout=100)  # should be a NOP if already started\n",
    "    dayPrediction = dayService.predict(dayData.to_numpy())\n",
    "\n",
    "    predictions = [trendPrediction, dayPrediction]\n",
    "    true = [data, dayData]\n",
    "\n",
    "    return predictions, true\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f649f4a7",
   "metadata": {},
   "source": [
    "## REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb426df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 191 ms (started: 2023-03-27 14:03:28 +02:00)\n"
     ]
    }
   ],
   "source": [
    "@step\n",
    "def checkDataQuality(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    data_quality_report = TestSuite(tests=[\n",
    "        TestNumberOfColumnsWithMissingValues(),\n",
    "        TestNumberOfRowsWithMissingValues(),\n",
    "        TestNumberOfConstantColumns(),\n",
    "        TestNumberOfDuplicatedRows(),\n",
    "        TestNumberOfDuplicatedColumns(),\n",
    "        DataQualityTestPreset(),\n",
    "#         DataStabilityTestPreset()\n",
    "    ])\n",
    "    data.rename(columns={\"price_diff\": \"target\"}, inplace=True)\n",
    "    data_quality_report.run(reference_data=None, current_data=data)\n",
    "    data_quality_report.save_html(\"data_quality_report.html\")\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "@step(\n",
    "    experiment_tracker=\"ireland_green_tracker\",\n",
    "    settings={\n",
    "        \"experiment_tracker.mlflow\": MLFlowExperimentTrackerSettings(\n",
    "            experiment_name=\"ireland_eval\",\n",
    "            nested=True,\n",
    "            tags={\"mlflow.runName\": \"inference_reports\"},\n",
    "        )\n",
    "    },\n",
    ")\n",
    "def publishResultAndReport(\n",
    "    target: pd.Series, features: pd.DataFrame, preds: list, trues: list\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    \n",
    "    targetSign = np.sign(target.mask(target <= 0, -1))\n",
    "    index = trues[0].index\n",
    "    graphs = []\n",
    "    \n",
    "    # -----      Trend Model -------------------------------\n",
    "    predTrend = pd.DataFrame(preds[0], index=index, columns=[\"prediction\"])\n",
    "    \n",
    "    pnL = (predTrend.squeeze() * target.loc[predTrend.index].squeeze())\n",
    "    graphs.append(go.Scatter(x=predTrend.index, \n",
    "               y=pnL.cumsum(),\n",
    "               mode='lines',\n",
    "               name=\"trend\"\n",
    "            )\n",
    "    )\n",
    "    \n",
    "    finalReportDFTrend = pd.concat(\n",
    "        [\n",
    "            targetSign.to_frame(\"target\").loc[index, :],\n",
    "            predTrend,\n",
    "            features.loc[index, :],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    reportTrend = Report(\n",
    "        metrics=[ClassificationPreset()]\n",
    "    )\n",
    "    reportTrend.run(\n",
    "        reference_data=finalReportDFTrend.loc[\n",
    "            finalReportDFTrend.index < \"2022-12-27\", :\n",
    "        ],\n",
    "        current_data=finalReportDFTrend.loc[\n",
    "            finalReportDFTrend.index >= \"2022-12-27\", :\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    mlflow.log_dict(predTrend.to_dict(\"records\"), \"results/trend/predictions.json\")\n",
    "    mlflow.log_dict(\n",
    "        finalReportDFTrend.loc[finalReportDFTrend.index < \"2022-12-27\", :].to_dict(\n",
    "            \"records\"\n",
    "        ),\n",
    "        \"results/trend/reference_data.json\",\n",
    "    )\n",
    "    mlflow.log_dict(\n",
    "        finalReportDFTrend.loc[finalReportDFTrend.index >= \"2022-12-27\", :].to_dict(\n",
    "            \"records\"\n",
    "        ),\n",
    "        \"results/trend/current_data.json\",\n",
    "    )\n",
    "    mlflow.log_dict(reportTrend.as_dict(), \"reports/trend/prediction_report.yml\")\n",
    "    reportTrend.save_html(\"trend_prediction_report.html\")\n",
    "    mlflow.log_artifact(\n",
    "        \"trend_prediction_report.html\",\n",
    "        \"reports/trend\",\n",
    "    )\n",
    "    # -----      DAY Model -------------------------------\n",
    "    \n",
    "    predDay = hourlySample(\n",
    "        pd.DataFrame(preds[1], index=trues[1].index, columns=[\"prediction\"])\n",
    "    )\n",
    "    \n",
    "    pnL = (predDay.squeeze() * target.loc[predDay.index].squeeze())\n",
    "    graphs.append(go.Scatter(x=predDay.index, \n",
    "               y=pnL.cumsum(),\n",
    "               mode='lines',\n",
    "               name=\"day\"\n",
    "            )\n",
    "    )\n",
    "    \n",
    "    finalReportDFDay = pd.concat(\n",
    "        [\n",
    "            targetSign.to_frame(\"target\").loc[index, :],\n",
    "            predDay,\n",
    "            features.loc[index, :],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    reportDay = Report(\n",
    "        metrics=[DataQualityPreset(), DataDriftPreset(), ClassificationPreset()]\n",
    "    )\n",
    "    reportDay.run(\n",
    "        reference_data=finalReportDFDay.loc[finalReportDFDay.index < \"2022-12-27\", :],\n",
    "        current_data=finalReportDFDay.loc[finalReportDFDay.index >= \"2022-12-27\", :],\n",
    "    )\n",
    "\n",
    "    mlflow.log_dict(predDay.to_frame().to_dict(\"records\"), \"results/day/predictions.json\")\n",
    "    mlflow.log_dict(\n",
    "        finalReportDFDay.loc[finalReportDFDay.index < \"2022-12-27\", :].to_dict(\n",
    "            \"records\"\n",
    "        ),\n",
    "        \"results/day/reference_data.json\",\n",
    "    )\n",
    "    mlflow.log_dict(\n",
    "        finalReportDFDay.loc[finalReportDFDay.index >= \"2022-12-27\", :].to_dict(\n",
    "            \"records\"\n",
    "        ),\n",
    "        \"results/day/current_data.json\",\n",
    "    )\n",
    "    mlflow.log_dict(reportDay.as_dict(), \"reports/day/prediction_report.yml\")\n",
    "    reportTrend.save_html(\"day_prediction_report.html\")\n",
    "    mlflow.log_artifact(\n",
    "        \"day_prediction_report.html\",\n",
    "        \"reports/day\",\n",
    "    )\n",
    "\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            predTrend.rename(columns={\"prediction\": \"trend_prediction\"}),\n",
    "            predDay.to_frame().rename(columns={\"prediction\": \"day_prediction\"}),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    fig = go.Figure(data = graphs)\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=1700,\n",
    "        height=500,\n",
    "#         legend=dict(groupclick=\"toggleitem\")\n",
    "    )\n",
    "    \n",
    "    mlflow.log_dict(df.to_dict(\"records\"), \"predictions.json\")\n",
    "    mlflow.log_figure(fig, f'pnl.html')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ddd36f",
   "metadata": {},
   "source": [
    "## PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fd47ccf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mRegistered pipeline \u001b[0m\u001b[33minference_pipeline\u001b[1;35m (version 4).\u001b[0m\n",
      "\u001b[1;35mRunning pipeline \u001b[0m\u001b[33minference_pipeline\u001b[1;35m on stack \u001b[0m\u001b[33mireland_green_deployment_stack\u001b[1;35m (caching disabled)\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mdates\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mdates\u001b[1;35m has finished in 0.846s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mprediction_service_loader\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mprediction_service_loader\u001b[1;35m has finished in 0.988s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mimporter\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mimporter\u001b[1;35m has finished in 4.536s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mqualityChecker\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mqualityChecker\u001b[1;35m has finished in 6.411s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mfilter_data\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mfilter_data\u001b[1;35m has finished in 2.115s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mprepare\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mprepare\u001b[1;35m has finished in 2.568s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mpredictor\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mStep \u001b[0m\u001b[33mpredictor\u001b[1;35m has finished in 2.954s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mpublisher\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mpublisher\u001b[1;35m has finished in 37.802s.\u001b[0m\n",
      "\u001b[1;35mPipeline run \u001b[0m\u001b[33minference_pipeline-2023_03_27-12_03_31_928466\u001b[1;35m has finished in 1m8s.\u001b[0m\n",
      "\u001b[1;35mDashboard URL: http://127.0.0.1:8237/workspaces/default/pipelines/7242b106-7cfb-4c2b-8a83-b26be488ebdd/runs\u001b[0m\n",
      "time: 1min 10s (started: 2023-03-27 14:03:29 +02:00)\n"
     ]
    }
   ],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "@pipeline(\n",
    "    enable_cache=False, name=\"ireland_eval\"\n",
    ")\n",
    "def inference_pipeline(\n",
    "    dates,\n",
    "    importer,\n",
    "    qualityChecker,\n",
    "    filter_data,\n",
    "    prepare,\n",
    "    prediction_service_loader,\n",
    "    predictor,\n",
    "    publisher\n",
    "):\n",
    "    \"\"\"Basic inference pipeline.\"\"\"\n",
    "    \n",
    "    (_stDate, _endDate) = dates()\n",
    "    data = importer(_stDate, _endDate)\n",
    "    qualityData = qualityChecker(data)\n",
    "    \n",
    "    target, features = filter_data(data, _stDate)\n",
    "    \n",
    "    X_test, X_day_test = prepare(features, _stDate, _endDate)\n",
    "    \n",
    "    trendService, dayService = prediction_service_loader()\n",
    "    predictions, trues = predictor(trendService, dayService, X_test, X_day_test)\n",
    "    \n",
    "    _ = publisher(target, features, predictions, trues)\n",
    "    \n",
    "\n",
    "# Initialize an inference pipeline run\n",
    "my_inference_pipeline = inference_pipeline(\n",
    "    dates = getDates(),\n",
    "    qualityChecker = checkDataQuality(),\n",
    "    importer = fetchData(),\n",
    "    filter_data= filteredData(), \n",
    "    prepare = prepareFeatures(),\n",
    "    prediction_service_loader=prediction_service_loader(),\n",
    "    predictor=predictor(),\n",
    "    publisher=publishResultAndReport()\n",
    ")\n",
    "\n",
    "my_inference_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3c410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fba391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.3px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
