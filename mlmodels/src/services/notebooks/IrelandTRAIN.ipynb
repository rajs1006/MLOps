{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad3fd711",
   "metadata": {},
   "source": [
    "## LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5242c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 513 ms (started: 2023-04-03 16:42:42 +02:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline \n",
    "#inline\n",
    "# %pdb\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "## setting path\n",
    "## set the base path\n",
    "# os.chdir(pathlib.Path(os.path.dirname(path[0])).parent.absolute())\n",
    "# ## set path to sys for Lambda function\n",
    "# sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0971b16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUnable to find ZenML repository in your current working directory (/home/sourabh/Resonanz/Algotrader/ConvexDSC/algotrader/src/services/ireland/notebooks) or any parent directories. If you want to use an existing repository which is in a different location, set the environment variable 'ZENML_REPOSITORY_PATH'. If you want to create a new repository, run \u001b[0m\u001b[33mzenml init\u001b[33m.\u001b[0m\n",
      "\u001b[1;35mRunning without an active repository root.\u001b[0m\n",
      "\u001b[2;36mConnected to the ZenML server: \u001b[0m\u001b[2;32m'http://127.0.0.1:8237'\u001b[0m\n",
      "\u001b[2;36mRunning with active workspace: \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mglobal\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "\u001b[2K\u001b[32mтаж\u001b[0m Setting the global active stack to 'ireland_green_deployment_stack'.....\n",
      "\u001b[1A\u001b[2KError: \u001b[31m\u001b[1m\"Stack 'ireland_green_deployment_stack' cannot be activated since it is not registered yet. Please register it first.\"\u001b[0m\n",
      "time: 4.65 s (started: 2023-04-03 16:42:43 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# !zenml data-validator register ireland_data_validator --flavor=evidently\n",
    "\n",
    "# # Register the MLflow experiment tracker\n",
    "# !zenml experiment-tracker register ireland_green_tracker --flavor=mlflow --tracking_uri=http://0.0.0.0:7009 --tracking_token=resonanz_ireland\n",
    "\n",
    "# !zenml orchestrator register ireland_green_orchestrator --flavor=airflow --local=True\n",
    "# # --flavor=local\n",
    "# # --flavor=airflow --local=True\n",
    "\n",
    "# # zenml model-registry register ireland_model_registry --flavor=mlflow\n",
    "\n",
    "# !zenml artifact-store register ireland_green_artifact_store --flavor=s3 --path=s3://bayaga/ireland\n",
    "\n",
    "# # Register the MLflow model deployer\n",
    "# !zenml model-deployer register ireland_green_dev --flavor=mlflow\n",
    "\n",
    "# # !zenml feature-store register ida_feast_store --flavor=feast --feast_repo=\"s3://bayaga/registry.pb\"\n",
    "\n",
    "# # Create a new stack that includes an MLflow experiment\n",
    "# !zenml stack register ireland_green_deployment_stack -e ireland_green_tracker \\\n",
    "#     -a ireland_green_artifact_store -o ireland_green_orchestrator -d ireland_green_dev \\\n",
    "#     -dv ireland_data_validator \n",
    "# #     -r ireland_model_registry\n",
    "# #-f ida_feast_store\n",
    "\n",
    "# # Set the new stack as active\n",
    "!zenml stack set ireland_green_deployment_stack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7204fde",
   "metadata": {},
   "source": [
    "## IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126f8753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (py.warnings._showwarnmsg) - /opt/conda/conda/envs/azba/lib/python3.9/site-packages/plotly/graph_objs/__init__.py:288: DeprecationWarning:\n",
      "\n",
      "distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\n",
      "\n",
      "WARNING:py.warnings:/opt/conda/conda/envs/azba/lib/python3.9/site-packages/plotly/graph_objs/__init__.py:288: DeprecationWarning:\n",
      "\n",
      "distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.44 s (started: 2023-03-27 16:57:23 +02:00)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "# import dtale\n",
    "# from prophet import Prophet\n",
    "from neuralprophet import NeuralProphet, set_log_level, set_random_seed\n",
    "set_log_level(\"ERROR\")\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from scipy.stats import t\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://0.0.0.0:7009\")\n",
    "# from mlflow import MlflowClient \n",
    "\n",
    "import dtale\n",
    "\n",
    "# from lightgbm import LGBMRegressor\n",
    "# import dtale\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import power_transform, scale\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from imblearn.ensemble import RUSBoostClassifier, EasyEnsembleClassifier, BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, accuracy_score\n",
    "from sklearn.metrics import explained_variance_score, max_error, r2_score, mean_absolute_error, f1_score\n",
    "\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsRegressor, NeighborhoodComponentsAnalysis\n",
    "# from lightgbm import LGBMClassifier\n",
    "# import lightgbm as lgb\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import VotingRegressor, GradientBoostingRegressor, RandomForestRegressor, VotingClassifier\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "from absl import logging as absl_logging\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "absl_logging.set_verbosity(-10000)\n",
    "\n",
    "import mlflow\n",
    "from zenml.steps import Output, step, StepContext\n",
    "\n",
    "\n",
    "import sqlalchemy\n",
    "import yaml\n",
    "from typing import Dict, Union\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import feast\n",
    "from feast import FeatureStore\n",
    "from feast.repo_config import RegistryConfig, RepoConfig\n",
    "from feast.infra.online_stores.redis import RedisOnlineStoreConfig\n",
    "\n",
    "from zenml.client import Client\n",
    "from zenml.integrations.mlflow.flavors.mlflow_experiment_tracker_flavor import (\n",
    "    MLFlowExperimentTrackerSettings,\n",
    ")\n",
    "from zenml.services import BaseService\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import (\n",
    "    DataQualityPreset,\n",
    "    DataDriftPreset,\n",
    "    ClassificationPreset,\n",
    ")\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metrics.base_metric import generate_column_metrics\n",
    "from evidently.metric_preset import DataDriftPreset, TargetDriftPreset\n",
    "from evidently.metrics import *\n",
    "\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.tests.base_test import generate_column_tests\n",
    "from evidently.test_preset import DataStabilityTestPreset, NoTargetPerformanceTestPreset, DataQualityTestPreset\n",
    "from evidently.tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38d194",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0b7a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.6 ms (started: 2023-03-27 16:57:28 +02:00)\n"
     ]
    }
   ],
   "source": [
    "def hourlySample(data):\n",
    "    d = data.copy()\n",
    "    d.index = pd.DatetimeIndex(d.index) \n",
    "    return d.reindex(pd.date_range(start=d.index.min(), periods = 24 * len(d), freq='h'), method='ffill').squeeze()\n",
    "\n",
    "def findOutliers(df):\n",
    "    s = df.copy()\n",
    "    q1 = s.quantile(0.25)\n",
    "    q3 = s.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    iqr_lower = q1 - 8 * iqr\n",
    "    iqr_upper = q3 + 8 * iqr\n",
    "#     outliers = s[(s < iqr_lower) | (s > iqr_upper)]\n",
    "    s[s < iqr_lower] = iqr_lower\n",
    "    s[s > iqr_upper] = iqr_upper\n",
    "    \n",
    "    return s\n",
    "\n",
    "def _scores(pred, true):\n",
    "    display(confusion_matrix(true.loc[pred.index], pred))\n",
    "    print(\"bAcc : \", bscore := balanced_accuracy_score(true.loc[pred.index], pred))\n",
    "    print(\"acc :\" , acc := accuracy_score(true.loc[pred.index], pred))\n",
    "    print(\"f1 :\", f1 := f1_score(true.loc[pred.index], pred))\n",
    "    print(\"report :\", classification_report(true.loc[pred.index], pred))\n",
    "    print(\"precision, recall\", prScore := precision_recall_fscore_support(true.loc[pred.index], pred, average='weighted'))\n",
    "    \n",
    "    return (bscore, acc, f1, prScore)\n",
    "\n",
    "def _scoresArr(pred, true, printVal=True):\n",
    "    bscore = balanced_accuracy_score(true, pred)\n",
    "    acc = accuracy_score(true, pred)\n",
    "    f1 = f1_score(true, pred)\n",
    "    prScore = precision_recall_fscore_support(true, pred, average='weighted')\n",
    "        \n",
    "    if printVal:\n",
    "        display(confusion_matrix(true, pred))\n",
    "        print(\"bAcc : \", bscore)\n",
    "        print(\"acc :\" , acc)\n",
    "        print(\"f1 :\", f1)\n",
    "        print(\"report :\", classification_report(true, pred))\n",
    "        print(\"precision, recall\", )\n",
    "    \n",
    "    return (bscore, acc, f1, prScore)\n",
    "\n",
    "def _scoresRegArr(actual, pred, printVal=True):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "        \n",
    "    if printVal:\n",
    "        print(\"rmse : \", rmse)\n",
    "        print(\"mae : \", mae)\n",
    "        print(\"r2 : \", r2)\n",
    "        \n",
    "    return (rmse, mae, r2)\n",
    "    \n",
    "def _toMultiIndex(d, name=None):\n",
    "    reformed_dict = {}\n",
    "    for outerKey, innerDict in d.items():\n",
    "        for innerKey, values in innerDict.items():\n",
    "            reformed_dict[(outerKey,\n",
    "                           innerKey)] = values\n",
    "\n",
    "    l = pd.DataFrame(reformed_dict)\n",
    "    display(l)\n",
    "    if name:\n",
    "        l.to_csv(name + \".csv\")\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d77d9c",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf021234",
   "metadata": {},
   "source": [
    "### Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0d1188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 82.5 ms (started: 2023-03-27 16:57:28 +02:00)\n"
     ]
    }
   ],
   "source": [
    "@step\n",
    "def getDates() -> Output(\n",
    "    stDate = str, endDate=str, dts=tuple\n",
    "):\n",
    "    stDate = \"2019-03-17\"\n",
    "    endDate = \"2023-03-20\" \n",
    "    \n",
    "#     st = '2022-03-16'\n",
    "    st = '2020-01-01'\n",
    "    trEnd = '2022-06-30'\n",
    "    tstStart = '2022-07-01'\n",
    "    pEnd = '2023-02-20'\n",
    "    \n",
    "    dts = (st, trEnd, tstStart, pEnd)\n",
    "    \n",
    "    return stDate, endDate, dts\n",
    "\n",
    "@step\n",
    "def fetchData(_stDate:str, _endDate:str, context: StepContext)-> Output(dataFrame=pd.DataFrame):\n",
    "    \n",
    "#     repoConfig = RepoConfig(\n",
    "#         registry=RegistryConfig(path=\"s3://bayaga/registry.pb\"),\n",
    "#         project=\"IDA\",\n",
    "#         provider=\"local\",\n",
    "#         offline_store=\"file\",\n",
    "#         online_store=RedisOnlineStoreConfig(connection_string=\"localhost:6379\"),\n",
    "#         entity_key_serialization_version=2,\n",
    "#     )\n",
    "#     store = FeatureStore(config=repoConfig)\n",
    "    repoPath = \"<path of feast repo>\"\n",
    "    store = FeatureStore(repo_path=repoPath)\n",
    "    \n",
    "    dd = [{'epoch_time' : int(pd.Timestamp(d.strftime('%Y-%m-%d %H:%M:%S')).timestamp())} for d in pd.date_range(start=_stDate, end=_endDate, freq='H')]\n",
    "\n",
    "    batch_features = store.get_online_features(\n",
    "        entity_rows=dd,\n",
    "        features=store.get_feature_service(\"ireland_data_service\", allow_cache=True),\n",
    "    ).to_df()\n",
    "    \n",
    "    df = batch_features.copy()\n",
    "    \n",
    "    df.set_index('epoch_time', inplace=True)\n",
    "    df.index = pd.to_datetime(df.index, unit='s')\n",
    "    df.index.name = None\n",
    "    \n",
    "    dataFrame = df.astype(float)\n",
    "    \n",
    "    # drop the first NaN value as that might be because of shift\n",
    "    dataFrame = dataFrame.iloc[dataFrame.notnull().all(axis=1).argmax() :]\n",
    "    dataFrame = dataFrame.groupby(by=dataFrame.index, as_index=True).mean()\n",
    "    \n",
    "    dataFrame = dataFrame.reindex(\n",
    "        pd.date_range(\n",
    "            start=dataFrame.index.min(), end=dataFrame.index.max(), freq=\"H\"\n",
    "        ),\n",
    "        fill_value=np.nan,\n",
    "    )\n",
    "    dataFrame = dataFrame.interpolate(method=\"bfill\", axis=0)\n",
    "    print(dataFrame.columns)\n",
    "    return dataFrame\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8799fb54",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bfe41f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 81.3 ms (started: 2023-03-27 16:57:28 +02:00)\n"
     ]
    }
   ],
   "source": [
    "# _stDate  = '2019-03-16'\n",
    "@step\n",
    "def filteredData(dataConcatenated : pd.DataFrame, _stDate : str) -> Output(targetSpreadWOOutlier=pd.DataFrame, features=pd.DataFrame):\n",
    "    data = dataConcatenated.loc[_stDate:][:-1]\n",
    "    \n",
    "    targetSpread = data['price_diff'].dropna().asfreq('h')\n",
    "    features = data.loc[targetSpread.index, ~data.columns.isin(['price_diff', 'ie_actual_wnd'])].asfreq('h')\n",
    "    \n",
    "    features = features.loc[: , ['day', 'weekday', 'year_day', 'week', 'month', 'quarter', 'lag1',\n",
    "       'lag2', 'lag3', 'lag4', 'lag5', 'lag6', 'lag7', 'lag30']]\n",
    "    \n",
    "    targetSpreadWOOutlier = findOutliers(targetSpread).to_frame()\n",
    "    \n",
    "    return targetSpreadWOOutlier, features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be97ec0",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ab4cdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 78.4 ms (started: 2023-03-27 16:57:28 +02:00)\n"
     ]
    }
   ],
   "source": [
    "@step\n",
    "def STLTransform(targetSpreadWOOutlier : pd.Series) -> Output(targetSpreadWOOutlierTrend=pd.Series):\n",
    "\n",
    "    s = targetSpreadWOOutlier.copy().asfreq(freq='H')\n",
    "    s.index = pd.to_datetime(s.index)\n",
    "\n",
    "    stl_result = STL(s).fit()\n",
    "    targetSpreadWOOutlierTrend = stl_result.trend\n",
    "    stl_result.plot()\n",
    "    \n",
    "    return targetSpreadWOOutlierTrend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c8b0e",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5dac225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 97.8 ms (started: 2023-03-27 16:57:28 +02:00)\n"
     ]
    }
   ],
   "source": [
    "@step\n",
    "def prepareSpreadTrend(targetSpreadWOOutlier : pd.Series, targetSpreadWOOutlierTrend: pd.Series, features : pd.DataFrame)-> Output(\n",
    "        targetSpreadSign=pd.DataFrame, targetSpreadTrendSign=pd.DataFrame, targetSpreadDaySign=pd.DataFrame, features=pd.DataFrame, dayFeatures=pd.DataFrame, featureColumns=list\n",
    "    ):\n",
    "\n",
    "    targetSpreadReal = targetSpreadWOOutlier.to_frame().copy().rename(columns = {'price_diff' : 'spread_real'})\n",
    "    targetSpreadSign = np.sign(targetSpreadReal.mask(targetSpreadReal <= 0, -1)).rename(columns = {'spread_real' : 'spread_sign'}).astype(int)\n",
    "    \n",
    "    #trend\n",
    "    targetSpreadTrendReal = targetSpreadWOOutlierTrend.copy().to_frame('spread_trend_real')\n",
    "    targetSpreadTrendSign = np.sign(targetSpreadTrendReal.mask(targetSpreadTrendReal <= 0, -1)).rename(columns = {'spread_trend_real' : 'spread_trend_sign'}).astype(int)\n",
    "\n",
    "    \n",
    "    targetSpreadDayReal = targetSpreadWOOutlier.to_frame().groupby([targetSpreadWOOutlier.index.date]).sum().rename(columns = {'price_diff' : 'spread_day_real'})\n",
    "    targetSpreadDayReal.index = pd.to_datetime(targetSpreadDayReal.index)\n",
    "    targetSpreadDaySign = np.sign(targetSpreadDayReal.mask(targetSpreadDayReal <= 0, -1)).rename(columns = {'spread_day_real' : 'spread_day_sign'}).astype(int)\n",
    "    \n",
    "    features = features.dropna()\n",
    "    dayFeatures = features.resample('D', axis=0).sum() \n",
    "    \n",
    "    featureColumns = list(features.columns)\n",
    "    \n",
    "    return targetSpreadSign, targetSpreadTrendSign, targetSpreadDaySign, features, dayFeatures, featureColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f399c",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "490f1a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 140 ms (started: 2023-03-27 16:57:29 +02:00)\n"
     ]
    }
   ],
   "source": [
    "@step\n",
    "def trainTestSplit(features:pd.DataFrame, targetSpreadSign:pd.DataFrame, targetSpreadTrendSign:pd.DataFrame, targetSpreadDaySign:pd.DataFrame, fetaureColumns:list, dates:tuple) -> Output(\n",
    "        X_train=pd.DataFrame, X_test=pd.DataFrame, y_train=pd.DataFrame, y_test=pd.DataFrame, \n",
    "        y_trend_train=pd.DataFrame, y_trend_test=pd.DataFrame, \n",
    "        X_day_train=pd.DataFrame, X_day_test=pd.DataFrame, y_train_day_sign=pd.DataFrame, y_test_day_sign=pd.DataFrame\n",
    "):\n",
    "\n",
    "    st, trEnd, tstStart, pEnd = dates\n",
    "    \n",
    "    X_train, X_test = (features.loc[st : trEnd, fetaureColumns], features.loc[tstStart : pEnd, fetaureColumns])\n",
    "    y_train, y_test = targetSpreadSign.loc[st : trEnd], targetSpreadSign.loc[tstStart : pEnd]\n",
    "    y_trend_train, y_trend_test = targetSpreadTrendSign.loc[st : trEnd], targetSpreadTrendSign.loc[tstStart : pEnd]\n",
    "    \n",
    "    X_day_train, X_day_test= (features.loc[st : trEnd, fetaureColumns].resample('D', axis=0).sum(), \n",
    "                          features.loc[tstStart : pEnd, fetaureColumns].resample('D', axis=0).sum())\n",
    "    y_train_day_sign, y_test_day_sign = targetSpreadDaySign.loc[st : trEnd], targetSpreadDaySign.loc[tstStart : pEnd]\n",
    "    \n",
    "    print(X_train)\n",
    "    print(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, y_trend_train, y_trend_test, X_day_train, X_day_test, y_train_day_sign, y_test_day_sign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9cc028",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b26f46",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb30a2",
   "metadata": {},
   "source": [
    "#### Trend and Day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22ac51bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 135 ms (started: 2023-03-27 16:57:29 +02:00)\n"
     ]
    }
   ],
   "source": [
    "@step\n",
    "def getTrendModelAndMetadata()-> Output(\n",
    "    modl=tuple, day=bool\n",
    "):\n",
    "    modl = ('trend_classifier',\n",
    "                  BalancedBaggingClassifier(estimator=LogisticRegression(random_state=42,solver='lbfgs', max_iter=1000), random_state=0))\n",
    "    day = False\n",
    "    \n",
    "    return modl, day\n",
    "\n",
    "@step\n",
    "def getDayModelAndMetadata()-> Output(\n",
    "    modl=tuple, day=bool\n",
    "):\n",
    "    modl = ('day_classifier',\n",
    "                  BalancedBaggingClassifier(estimator=LogisticRegression(random_state=42,solver='lbfgs', max_iter=1000), random_state=0))\n",
    "    day = True\n",
    "    \n",
    "    return modl, day\n",
    "\n",
    "@step(\n",
    "    experiment_tracker=\"ireland_green_tracker\",\n",
    "    settings={\n",
    "        \"experiment_tracker.mlflow\": MLFlowExperimentTrackerSettings(\n",
    "            experiment_name=\"ireland_eval\",\n",
    "            nested=True,\n",
    "            tags={\"mlflow.runName\": \"evaluation\"},\n",
    "        )\n",
    "    },\n",
    ")\n",
    "def trainModel(X:pd.DataFrame, Y:pd.DataFrame, modl:tuple, dayType:bool)-> Output(\n",
    "    finalTrain = pd.DataFrame, model = Pipeline\n",
    "):\n",
    "    finalTrain = pd.DataFrame(index=Y.index)\n",
    "\n",
    "    name, clf = modl\n",
    "\n",
    "    p = [(\"transform\", StandardScaler()), (\"model\", clf)]\n",
    "    model = Pipeline(p)\n",
    "\n",
    "    mlflow.set_tag(\"mlflow.runName\", name)\n",
    "    mlflow.sklearn.autolog(registered_model_name=f\"{name}\")\n",
    "\n",
    "    model.fit(X.to_numpy(), Y.squeeze().to_numpy())\n",
    "\n",
    "    score = model.score(X.to_numpy(), Y.squeeze().to_numpy())\n",
    "    trainPred = model.predict(X.to_numpy())\n",
    "\n",
    "    mlflow.log_metric(\"training score\", score)\n",
    "\n",
    "    if dayType:\n",
    "        finalTrain[\"day_pred\"] = hourlySample(\n",
    "            pd.DataFrame(trainPred.astype(int), index=Y.index)\n",
    "        )\n",
    "    else:\n",
    "        finalTrain[\"pred\"] = trainPred.astype(int)\n",
    "\n",
    "    return finalTrain, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288f675",
   "metadata": {},
   "source": [
    "#### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09445a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 154 ms (started: 2023-03-27 16:57:29 +02:00)\n"
     ]
    }
   ],
   "source": [
    "@step\n",
    "def prepareCombinedFeatures(trendTrainPred:pd.DataFrame, dayTrainPred:pd.DataFrame, features:pd.DataFrame)-> Output(\n",
    "    combinedFeatures=pd.DataFrame\n",
    "):\n",
    "    \n",
    "    combinedFeatures = pd.concat([trendTrainPred, dayTrainPred, features], axis=1)\n",
    "    \n",
    "    return combinedFeatures\n",
    "\n",
    "@step\n",
    "def getCombinedModelAndMetadata()-> Output(\n",
    "    modl=tuple\n",
    "):\n",
    "    modl = ('combined_classifier',\n",
    "                  BalancedBaggingClassifier(estimator=CategoricalNB(fit_prior=False, force_alpha=True), random_state=0, verbose=False))\n",
    "    \n",
    "    return modl\n",
    "            \n",
    "#, experiment_tracker=\"ireland_green_tracker\"\n",
    "@step(enable_cache=False)\n",
    "def trainCombinedModel(X:pd.DataFrame, Y:pd.DataFrame, modl:tuple)-> Output(\n",
    "    finalTrain = pd.DataFrame, model = Pipeline\n",
    "):\n",
    "    datas = []\n",
    "    \n",
    "    finalTrain = pd.DataFrame(index = Y.index)\n",
    "    \n",
    "    name, clf = modl\n",
    "            \n",
    "    p = [\n",
    "      ('transform', StandardScaler()),\n",
    "      ('model', clf)\n",
    "    ]\n",
    "    model = Pipeline(p)\n",
    "\n",
    "#     mlflow.sklearn.autolog(registered_model_name=f\"{name}\") \n",
    "    model.fit(X, Y.squeeze())\n",
    "\n",
    "    score = model.score(X, Y.squeeze())\n",
    "    trainPred = model.predict(X)\n",
    "    \n",
    "#     mlflow.sklearn.log_model(\n",
    "#         model, f\"models/{name}\",\n",
    "#         registered_model_name=f\"{name}\"\n",
    "#     )\n",
    "    \n",
    "    finalTrain['combined_pred'] = trainPred.astype(int)\n",
    "    \n",
    "    return finalTrain, model, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d0f3c",
   "metadata": {},
   "source": [
    "## REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba5a29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 154 ms (started: 2023-03-27 16:57:29 +02:00)\n"
     ]
    }
   ],
   "source": [
    "@step\n",
    "def checkDataQuality(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    data_quality_report = TestSuite(tests=[\n",
    "        TestNumberOfColumnsWithMissingValues(),\n",
    "        TestNumberOfRowsWithMissingValues(),\n",
    "        TestNumberOfConstantColumns(),\n",
    "        TestNumberOfDuplicatedRows(),\n",
    "        TestNumberOfDuplicatedColumns(),\n",
    "        DataQualityTestPreset(),\n",
    "#         DataStabilityTestPreset()\n",
    "    ])\n",
    "    dataQuality = data.rename(columns={\"price_diff\": \"target\"})\n",
    "    data_quality_report.run(reference_data=None, current_data=dataQuality)\n",
    "    data_quality_report.save_html(\"data_quality_report.html\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e005071f",
   "metadata": {},
   "source": [
    "## PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d499d13",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('azba')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n azba ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "from zenml.integrations.mlflow.steps import (\n",
    "    MLFlowDeployerParameters, \n",
    "    mlflow_model_deployer_step\n",
    ")\n",
    "\n",
    "@step\n",
    "def deployment_decision() -> bool:\n",
    "    return True\n",
    "\n",
    "@pipeline(enable_cache=False)\n",
    "def eval_pipeline(\n",
    "    dates,\n",
    "    importer,\n",
    "    qualityChecker,\n",
    "    filterData,\n",
    "    transformer,\n",
    "    prepare,\n",
    "    splitter,\n",
    "    modelAndMetadataTrend,\n",
    "    modelAndMetadataDay,\n",
    "    trendTrainer,\n",
    "    dayTrainer,\n",
    "    deploymentDecision, \n",
    "    trendModelDeployer,\n",
    "    dayModelDeployer\n",
    "):\n",
    "    \"\"\"Train and deploy a model with MLflow.\"\"\"\n",
    "    _stDate, _endDate, dts = dates()\n",
    "    data = importer(_stDate, _endDate)\n",
    "    qualityData = qualityChecker(data)\n",
    "    \n",
    "    target, features = filterData(qualityData, _stDate)\n",
    "    trendTarget = transformer(target)\n",
    "    targetSpreadSign, targetSpreadTrendSign, targetSpreadDaySign, features, dayFeatures, fetaureColumns = prepare(target, trendTarget, features)\n",
    "\n",
    "    X_train, X_test, y_train, y_test, y_trend_train, y_trend_test, X_day_train, X_day_test, y_train_day_sign, y_test_day_sign = splitter(features, targetSpreadSign, targetSpreadTrendSign, targetSpreadDaySign, fetaureColumns, dts)\n",
    "    \n",
    "    modlTrend, dayTrend = modelAndMetadataTrend()\n",
    "    trendTrainPred, trendModel = trendTrainer(X=X_train, Y=y_trend_train, modl=modlTrend, dayType=dayTrend)\n",
    "    \n",
    "    modlDay, day = modelAndMetadataDay()\n",
    "    dayTrainPred, dayModel = dayTrainer(X=X_day_train, Y=y_train_day_sign, modl=modlDay, dayType=day)\n",
    "    \n",
    "    shouldDeploy = deploymentDecision()\n",
    "    trendModelDeployer(shouldDeploy, trendModel)\n",
    "    dayModelDeployer(shouldDeploy, dayModel)# new\n",
    "    \n",
    "eval_pipeline(\n",
    "    dates=getDates(),\n",
    "    importer=fetchData(),\n",
    "    qualityChecker = checkDataQuality(),\n",
    "    filterData = filteredData(),\n",
    "    transformer = STLTransform(),\n",
    "    prepare = prepareSpreadTrend(),\n",
    "    splitter = trainTestSplit(), \n",
    "    modelAndMetadataTrend=getTrendModelAndMetadata(),\n",
    "    modelAndMetadataDay=getDayModelAndMetadata(),\n",
    "    trendTrainer = trainModel().configure('trendTrainer'),\n",
    "    dayTrainer = trainModel().configure('dayTrainer'),\n",
    "    deploymentDecision=deployment_decision(),\n",
    "    trendModelDeployer=mlflow_model_deployer_step(\n",
    "        MLFlowDeployerParameters(timeout=500, run_name='trend_classifier')\n",
    "    ).configure('trendModelDeployer'),\n",
    "    dayModelDeployer=mlflow_model_deployer_step(\n",
    "        MLFlowDeployerParameters(timeout=500, run_name='day_classifier')\n",
    "    ).configure('dayModelDeployer')\n",
    ").run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('azba')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "409.575px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "308e0aebe9af7a0d0d4ce8829c4ec0a1fc4602d6811a3efb449c21a6ff193d68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
